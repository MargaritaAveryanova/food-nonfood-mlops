import os
import json
import numpy as np
import tensorflow as tf
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
from utils import load_params, setup_mlflow_experiment_safe, convert_numpy_types
import mlflow
import mlflow.sklearn
import joblib

def load_model(model_type: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞"""
    if model_type in ['cnn', 'transfer_learning']:
        model_path = f"models/{model_type}_model.keras"
        return tf.keras.models.load_model(model_path)
    else:  # random_forest
        model_path = f"models/{model_type}_model.joblib"
        return joblib.load(model_path)

def convert_numpy_types(obj):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy —Ç–∏–ø–æ–≤ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è JSON"""
    if isinstance(obj, (np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, (np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

def evaluate_model(model, X_test, y_test, model_type: str):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
    if model_type in ['cnn', 'transfer_learning']:
        # –î–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π - –ø–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ 2 –∑–Ω–∞—á–µ–Ω–∏—è
        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
        y_pred_proba = model.predict(X_test)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤—Ä—É—á–Ω—É—é
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        test_precision = precision_score(y_test, y_pred, zero_division=0)
        test_recall = recall_score(y_test, y_pred, zero_division=0)
        test_f1 = f1_score(y_test, y_pred, zero_division=0)
        
        metrics = {
            "test_accuracy": float(test_accuracy),
            "test_loss": float(test_loss),
            "test_precision": float(test_precision),
            "test_recall": float(test_recall),
            "test_f1_score": float(test_f1)
        }
    else:
        # –î–ª—è Random Forest
        X_test_flat = X_test.reshape(X_test.shape[0], -1)
        y_pred = model.predict(X_test_flat)
        y_pred_proba = model.predict_proba(X_test_flat)[:, 1]
        
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
        
        metrics = {
            "test_accuracy": float(accuracy_score(y_test, y_pred)),
            "test_precision": float(precision_score(y_test, y_pred, zero_division=0)),
            "test_recall": float(recall_score(y_test, y_pred, zero_division=0)),
            "test_f1_score": float(f1_score(y_test, y_pred, zero_division=0))
        }
    
    # ROC-AUC
    from sklearn.metrics import roc_auc_score
    try:
        if model_type != 'random_forest':
            roc_auc = roc_auc_score(y_test, y_pred_proba)
        else:
            roc_auc = roc_auc_score(y_test, y_pred_proba)
        metrics["test_roc_auc"] = float(roc_auc)
    except:
        metrics["test_roc_auc"] = 0.0
    
    return metrics, y_pred, y_pred_proba

def plot_confusion_matrix(y_true, y_pred, model_name: str):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Non-Food', 'Food'],
                yticklabels=['Non-Food', 'Food'])
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    os.makedirs("plots", exist_ok=True)
    plot_path = f"plots/confusion_matrix_{model_name}.png"
    plt.savefig(plot_path)
    plt.close()
    
    return plot_path

def plot_roc_curve(y_true, y_pred_proba, model_name: str):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC –∫—Ä–∏–≤–æ–π"""
    from sklearn.metrics import roc_curve, auc
    
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model_name}')
    plt.legend(loc="lower right")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    plot_path = f"plots/roc_curve_{model_name}.png"
    plt.savefig(plot_path)
    plt.close()
    
    return plot_path, roc_auc

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏"""
    params = load_params()
    model_type = params['training']['current_model']
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
    try:
        experiment_id = setup_mlflow_experiment_safe()
        if experiment_id:
            with mlflow.start_run(run_name=f"evaluate_{model_type}"):
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                X_test = np.load("data/processed/test/X_test.npy")
                y_test = np.load("data/processed/test/y_test.npy")
                
                print(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: {model_type}")
                print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {X_test.shape}")
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
                model = load_model(model_type)
                
                # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
                metrics, y_pred, y_pred_proba = evaluate_model(model, X_test, y_test, model_type)
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                cm_plot_path = plot_confusion_matrix(y_test, y_pred, model_type)
                roc_plot_path, roc_auc = plot_roc_curve(y_test, y_pred_proba, model_type)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                metrics["test_roc_auc"] = float(roc_auc)
                
                # –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Å–µ—Ö numpy —Ç–∏–ø–æ–≤
                metrics = convert_numpy_types(metrics)
                class_report = convert_numpy_types(class_report)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ –æ—Ç—á–µ—Ç–æ–≤
                os.makedirs("metrics", exist_ok=True)
                
                with open("metrics/test_metrics.json", "w") as f:
                    json.dump(metrics, f, indent=2)
                
                with open("metrics/classification_report.json", "w") as f:
                    json.dump(class_report, f, indent=2)
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
                mlflow.log_metrics(metrics)
                mlflow.log_artifact("metrics/test_metrics.json")
                mlflow.log_artifact("metrics/classification_report.json")
                mlflow.log_artifact(cm_plot_path)
                mlflow.log_artifact(roc_plot_path)
                
                # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                print("\n" + "="*50)
                print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
                print("="*50)
                for metric, value in metrics.items():
                    print(f"{metric}: {value:.4f}")
                print("="*50)
                
                print("\n–û–¢–ß–ï–¢ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
                print(classification_report(y_test, y_pred, target_names=['Non-Food', 'Food'], zero_division=0))
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å MLflow")
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ MLflow: {e}")
        print("üîß –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –±–µ–∑ MLflow...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        X_test = np.load("data/processed/test/X_test.npy")
        y_test = np.load("data/processed/test/y_test.npy")
        
        print(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: {model_type}")
        print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {X_test.shape}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model = load_model(model_type)
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        metrics, y_pred, y_pred_proba = evaluate_model(model, X_test, y_test, model_type)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        cm_plot_path = plot_confusion_matrix(y_test, y_pred, model_type)
        roc_plot_path, roc_auc = plot_roc_curve(y_test, y_pred_proba, model_type)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        metrics["test_roc_auc"] = float(roc_auc)
        
        # –û—Ç—á–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        class_report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Å–µ—Ö numpy —Ç–∏–ø–æ–≤
        metrics = convert_numpy_types(metrics)
        class_report = convert_numpy_types(class_report)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ –æ—Ç—á–µ—Ç–æ–≤
        os.makedirs("metrics", exist_ok=True)
        
        with open("metrics/test_metrics.json", "w") as f:
            json.dump(metrics, f, indent=2)
        
        with open("metrics/classification_report.json", "w") as f:
            json.dump(class_report, f, indent=2)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "="*50)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
        print("="*50)
        for metric, value in metrics.items():
            print(f"{metric}: {value:.4f}")
        print("="*50)
        
        print("\n–û–¢–ß–ï–¢ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
        print(classification_report(y_test, y_pred, target_names=['Non-Food', 'Food'], zero_division=0))

if __name__ == "__main__":
    main()