import os
import json
import numpy as np
import tensorflow as tf
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from utils import load_params, setup_mlflow_experiment_safe, log_params_to_mlflow, create_model_checkpoint_callback
import mlflow
import mlflow.sklearn
import joblib

def convert_numpy_types(obj):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy —Ç–∏–ø–æ–≤ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è JSON"""
    if isinstance(obj, (np.float32, np.float64, np.float16)):
        return float(obj)
    elif isinstance(obj, (np.int32, np.int64, np.int16, np.int8)):
        return int(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    else:
        return obj

def create_cnn_model(input_shape, params):
    """–°–æ–∑–¥–∞–Ω–∏–µ CNN –º–æ–¥–µ–ª–∏"""
    model = tf.keras.Sequential()
    
    # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
    filters = params.get('filters', [32, 64, 128])
    for i, filter_size in enumerate(filters):
        if i == 0:
            model.add(tf.keras.layers.Conv2D(
                filter_size, 
                params.get('kernel_size', 3), 
                activation='relu', 
                input_shape=input_shape
            ))
        else:
            model.add(tf.keras.layers.Conv2D(
                filter_size, 
                params.get('kernel_size', 3), 
                activation='relu'
            ))
        model.add(tf.keras.layers.MaxPooling2D(params.get('pool_size', 2)))
        model.add(tf.keras.layers.BatchNormalization())
    
    # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(params.get('dense_units', 128), activation='relu'))
    model.add(tf.keras.layers.Dropout(params.get('dropout_rate', 0.5)))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=params.get('learning_rate', 0.001)),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def create_transfer_learning_model(input_shape, params):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Transfer Learning"""
    # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
    if params.get('base_model', "MobileNetV2") == "MobileNetV2":
        base_model = tf.keras.applications.MobileNetV2(
            weights=params.get('weights', "imagenet"),
            include_top=params.get('include_top', False),
            input_shape=input_shape
        )
    
    # –ó–∞–º–æ—Ä–æ–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    base_model.trainable = False
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–ª–æ–µ–≤
    model = tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(params.get('dense_units', 128), activation='relu'),
        tf.keras.layers.Dropout(params.get('dropout_rate', 0.3)),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    
    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=params.get('learning_rate', 0.0001)),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def train_random_forest(X_train, y_train, params):
    """–û–±—É—á–µ–Ω–∏–µ Random Forest –º–æ–¥–µ–ª–∏"""
    # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RF
    X_train_flat = X_train.reshape(X_train.shape[0], -1)
    
    model = RandomForestClassifier(
        n_estimators=params['n_estimators'],
        max_depth=params['max_depth'],
        random_state=params['random_state']
    )
    
    model.fit(X_train_flat, y_train)
    return model

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    params = load_params()
    model_type = params['training']['current_model']
    model_params = params['models'][model_type]
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
    try:
        experiment_id = setup_mlflow_experiment_safe()
        if experiment_id:
            with mlflow.start_run(run_name=f"train_{model_type}") as run:
                print(f"üéØ MLflow Run ID: {run.info.run_id}")
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                log_params_to_mlflow(params)
                mlflow.set_tag("model_type", model_type)
                mlflow.set_tag("dataset", "food_vs_nonfood")
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                X_train = np.load("data/processed/train/X_train.npy")
                y_train = np.load("data/processed/train/y_train.npy")
                X_val = np.load("data/processed/validation/X_val.npy")
                y_val = np.load("data/processed/validation/y_val.npy")
                
                print(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_type}")
                print(f"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {X_train.shape}")
                
                # –û–±—É—á–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
                if model_type in ['cnn', 'transfer_learning']:
                    # –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏
                    input_shape = X_train.shape[1:]
                    
                    if model_type == 'cnn':
                        model = create_cnn_model(input_shape, model_params)
                        mlflow.set_tag("architecture", "custom_cnn")
                    else:
                        model = create_transfer_learning_model(input_shape, model_params)
                        mlflow.set_tag("architecture", "transfer_learning")
                    
                    # Callbacks
                    callbacks = [
                        create_model_checkpoint_callback(),
                        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
                    ]
                    
                    # –û–±—É—á–µ–Ω–∏–µ
                    history = model.fit(
                        X_train, y_train,
                        batch_size=params['data']['batch_size'],
                        epochs=model_params['epochs'],
                        validation_data=(X_val, y_val),
                        callbacks=callbacks,
                        verbose=1
                    )
                    
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –ö–ê–ñ–î–û–ô —ç–ø–æ—Ö–∏
                    for epoch in range(len(history.history['accuracy'])):
                        mlflow.log_metric("train_accuracy", history.history['accuracy'][epoch], step=epoch)
                        mlflow.log_metric("train_loss", history.history['loss'][epoch], step=epoch)
                        mlflow.log_metric("val_accuracy", history.history['val_accuracy'][epoch], step=epoch)
                        mlflow.log_metric("val_loss", history.history['val_loss'][epoch], step=epoch)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    model_path = f"models/{model_type}_model.keras"
                    model.save(model_path)
                    mlflow.keras.log_model(model, "model")
                    
                    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)
                    
                    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                    training_metrics = {
                        "final_val_accuracy": float(val_accuracy),
                        "final_val_loss": float(val_loss),
                        "final_epoch": len(history.history['loss'])
                    }
                    
                else:  # Random Forest
                    mlflow.set_tag("architecture", "random_forest")
                    model = train_random_forest(X_train, y_train, model_params)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ - –°–û–ó–î–ê–ï–ú –ü–ê–ü–ö–£ –ü–†–ï–ñ–î–ï
                    os.makedirs("models", exist_ok=True)  # ‚Üê –î–û–ë–ê–í–¨–¢–ï –≠–¢–£ –°–¢–†–û–ö–£
                    model_path = f"models/{model_type}_model.joblib"
                    import joblib
                    joblib.dump(model, model_path)
                    mlflow.sklearn.log_model(model, "model")
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    X_val_flat = X_val.reshape(X_val.shape[0], -1)
                    y_val_pred = model.predict(X_val_flat)
                    
                    from sklearn.metrics import accuracy_score
                    
                    val_accuracy = accuracy_score(y_val, y_val_pred)
                    
                    training_metrics = {
                        "final_val_accuracy": float(val_accuracy)
                    }
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy —Ç–∏–ø–æ–≤ –¥–ª—è JSON
                training_metrics = convert_numpy_types(training_metrics)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                os.makedirs("metrics", exist_ok=True)
                with open("metrics/training_metrics.json", "w") as f:
                    json.dump(training_metrics, f, indent=2)
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –§–ò–ù–ê–õ–¨–ù–´–• –º–µ—Ç—Ä–∏–∫
                for metric, value in training_metrics.items():
                    mlflow.log_metric(metric, value)
                
                mlflow.log_artifact("metrics/training_metrics.json")
                
                print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. Validation Accuracy: {training_metrics.get('final_val_accuracy', 'N/A'):.4f}")
        else:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å MLflow")
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ MLflow: {e}")
        print("üîß –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ MLflow...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_train = np.load("data/processed/train/X_train.npy")
        y_train = np.load("data/processed/train/y_train.npy")
        X_val = np.load("data/processed/validation/X_val.npy")
        y_val = np.load("data/processed/validation/y_val.npy")
        
        print(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_type}")
        print(f"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {X_train.shape}")
        
        # –û–±—É—á–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
        if model_type in ['cnn', 'transfer_learning']:
            # –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏
            input_shape = X_train.shape[1:]
            
            if model_type == 'cnn':
                model = create_cnn_model(input_shape, model_params)
            else:
                model = create_transfer_learning_model(input_shape, model_params)
            
            # Callbacks
            callbacks = [
                create_model_checkpoint_callback(),
                tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
            ]
            
            # –û–±—É—á–µ–Ω–∏–µ
            history = model.fit(
                X_train, y_train,
                batch_size=params['data']['batch_size'],
                epochs=model_params['epochs'],
                validation_data=(X_val, y_val),
                callbacks=callbacks,
                verbose=1
            )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model_path = f"models/{model_type}_model.keras"
            model.save(model_path)
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            training_metrics = {
                "final_val_accuracy": float(val_accuracy),
                "final_val_loss": float(val_loss),
                "final_epoch": len(history.history['loss'])
            }
            
        else:  # Random Forest
            mlflow.set_tag("architecture", "random_forest")
            model = train_random_forest(X_train, y_train, model_params)
            
            # –°–û–ó–î–ê–ï–ú –ü–ê–ü–ö–£ –ü–†–ï–ñ–î–ï –°–û–•–†–ê–ù–ï–ù–ò–Ø
            os.makedirs("models", exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model_path = f"models/{model_type}_model.joblib"
            import joblib
            joblib.dump(model, model_path)
            mlflow.sklearn.log_model(model, "model")
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            X_val_flat = X_val.reshape(X_val.shape[0], -1)
            y_val_pred = model.predict(X_val_flat)
            
            from sklearn.metrics import accuracy_score
            
            val_accuracy = accuracy_score(y_val, y_val_pred)
            
            training_metrics = {
                "final_val_accuracy": float(val_accuracy)
            }
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è numpy —Ç–∏–ø–æ–≤ –¥–ª—è JSON
        training_metrics = convert_numpy_types(training_metrics)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        os.makedirs("metrics", exist_ok=True)
        with open("metrics/training_metrics.json", "w") as f:
            json.dump(training_metrics, f, indent=2)
        
        print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. Validation Accuracy: {training_metrics.get('final_val_accuracy', 'N/A'):.4f}")

if __name__ == "__main__":
    main()